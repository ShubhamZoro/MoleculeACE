#Hyperparameters for the ChemBerta Transformer regressor

# N epochs with early stopping of Transformer models
epochs: 100
early_stopping_patience: 10
# Validation split for all deep learning methods
val_split: 0.1
# Augment smiles n times
augmentation: 10
#  batch size
batch_size: 32
# starting learning rate
lr: 0.0005
# this is the max length of a smiles
max_len_model: 200
# save a model every 'period' epochs
period: 2
#  is this core of ChemBerta trainable?
freeze_core: true
